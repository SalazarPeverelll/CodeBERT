{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(code_tokens_strs) 251820\n",
      "len(nl_tokens_strs) 251820\n",
      "Found existing tokenized files, loading them...\n",
      "len(idx_list) 49\n",
      "len(tokenized_id_data) 251820\n",
      "Loaded 170295 existing student-teacher pairs\n",
      "Processing training data with unique symbols/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling.\n",
      "Finished loading/processing unique symbols for tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import wordninja\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 将索引序列转换为区间格式\n",
    "def convert_to_intervals(indices):\n",
    "    \"\"\"将索引列表转换为区间格式\"\"\"\n",
    "    if not indices:\n",
    "        return []\n",
    "    \n",
    "    intervals = []\n",
    "    start = indices[0]\n",
    "    end = indices[0]\n",
    "    \n",
    "    for i in range(1, len(indices)):\n",
    "        if indices[i] == end + 1:\n",
    "            end = indices[i]\n",
    "        else:\n",
    "            intervals.append(start)\n",
    "            intervals.append(end)\n",
    "            start = indices[i]\n",
    "            end = indices[i]\n",
    "    \n",
    "    intervals.append(start)\n",
    "    intervals.append(end)\n",
    "    return intervals\n",
    "\n",
    "# Function to further tokenize code tokens and track token mappings\n",
    "def further_tokenize(tokens):\n",
    "    result = []\n",
    "    token_map = {}  # Map non-comment token index to list of split token indices\n",
    "    result_idx = 0\n",
    "    map_idx = 0  # Separate counter for non-comment tokens\n",
    "    \n",
    "    # Track function name after def\n",
    "    function_name = None\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        # Skip comment tokens starting with #\n",
    "        if token.startswith('#'):\n",
    "            continue\n",
    "            \n",
    "        # Track starting index for this original token\n",
    "        start_idx = result_idx\n",
    "        \n",
    "        # If previous token was 'def', store current token as function name\n",
    "        prev_token = tokens[i-1] if i > 0 else None\n",
    "        if prev_token == 'def':\n",
    "            function_name = token\n",
    "        \n",
    "        # If token is a single special character, add it directly\n",
    "        if len(token) == 1 and not (token.isalnum() or token == '_'):\n",
    "            # Special case: if this is a * and previous token was also *, reuse previous map_idx\n",
    "            if token == '*' and result and result[-1] == '*':\n",
    "                if map_idx > 0:  # Make sure we have a previous index to reference\n",
    "                    token_map[map_idx-1].append(result_idx)\n",
    "                    result.append(token)\n",
    "                    result_idx += 1\n",
    "                    continue\n",
    "            \n",
    "            result.append(token)\n",
    "            token_map[map_idx] = [result_idx]\n",
    "            result_idx += 1\n",
    "            map_idx += 1\n",
    "            continue\n",
    "            \n",
    "        # Check if token should be kept intact (API call or function name)\n",
    "        prev_token = tokens[i-1] if i > 0 else None\n",
    "        next_token = tokens[i+1] if i < len(tokens)-1 else None\n",
    "        \n",
    "        keep_intact = False\n",
    "        if (prev_token == '.' or next_token == '.' or next_token == '('):\n",
    "            # Exception: split if this is a function definition name or matches function name\n",
    "            if prev_token != 'def' and token != function_name:\n",
    "                keep_intact = True\n",
    "                \n",
    "        if keep_intact:\n",
    "            result.append(token)\n",
    "            token_map[map_idx] = [result_idx]\n",
    "            result_idx += 1\n",
    "            map_idx += 1\n",
    "            continue\n",
    "            \n",
    "        # Split by special characters while keeping them\n",
    "        parts = []\n",
    "        current = ''\n",
    "        for char in token:\n",
    "            if char.isalnum() or char == '_':\n",
    "                current += char\n",
    "            else:\n",
    "                if current:\n",
    "                    parts.append(current)\n",
    "                    current = ''\n",
    "                if char != ' ':  # Keep all special characters except spaces\n",
    "                    parts.append(char)\n",
    "        if current:\n",
    "            parts.append(current)\n",
    "            \n",
    "        # Process each part\n",
    "        for part in parts:\n",
    "            if '_' in part:\n",
    "                # Handle underscore-separated parts\n",
    "                if part.startswith('_'):\n",
    "                    result.append('_')\n",
    "                    result_idx += 1\n",
    "                    \n",
    "                subparts = [p for p in part.split('_') if p != '']\n",
    "                \n",
    "                for j, subpart in enumerate(subparts):\n",
    "                    if j > 0:\n",
    "                        result.append('_')\n",
    "                        result_idx += 1\n",
    "                    if subpart.strip('_'):\n",
    "                        split_subparts = wordninja.split(subpart)\n",
    "                        result.extend(p for p in split_subparts if p)\n",
    "                        result_idx += len([p for p in split_subparts if p])\n",
    "                \n",
    "                if part.endswith('_'):\n",
    "                    result.append('_')\n",
    "                    result_idx += 1\n",
    "            else:\n",
    "                # Handle other parts\n",
    "                split_parts = wordninja.split(part)\n",
    "                result.extend(p for p in split_parts if p)\n",
    "                result_idx += len([p for p in split_parts if p])\n",
    "                \n",
    "        # Map non-comment token to range of split tokens\n",
    "        token_map[map_idx] = list(range(start_idx, result_idx))\n",
    "        map_idx += 1\n",
    "                    \n",
    "    return result, token_map\n",
    "\n",
    "# Function to add unique symbols to repeated tokens\n",
    "def add_unique_symbols(tokens, is_comment=True):\n",
    "    # Track token counts and processed tokens\n",
    "    token_counts = {}\n",
    "    processed_tokens = []\n",
    "    \n",
    "    # First pass - count occurrences\n",
    "    for token in tokens:\n",
    "        if token in token_counts:\n",
    "            token_counts[token] += 1\n",
    "        else:\n",
    "            token_counts[token] = 1\n",
    "            \n",
    "    # Second pass - add symbols to repeated tokens\n",
    "    token_seen = {}\n",
    "    for token in tokens:\n",
    "        if token_counts[token] > 1:\n",
    "            # Track occurrence number for this token\n",
    "            if token not in token_seen:\n",
    "                token_seen[token] = 1\n",
    "            else:\n",
    "                token_seen[token] += 1\n",
    "                \n",
    "            # Add triangle (▲) for comments, square (■) for code\n",
    "            symbol = \"▲\" if is_comment else \"■\"\n",
    "            processed_tokens.append(f\"{token}{symbol}{token_seen[token]}\")\n",
    "        else:\n",
    "            processed_tokens.append(token)\n",
    "            \n",
    "    return processed_tokens\n",
    "\n",
    "# # set openai environ and key\n",
    "# os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "# os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-5QMoD9yjiyQb8hQcymsFeKhvh0rhkdJ1GiwRknz0rxtVgZg0\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://api.key77qiqi.cn/v1\"\n",
    "\n",
    "# load all training data\n",
    "# 文件路径\n",
    "file_path = '/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/train.jsonl'\n",
    "\n",
    "# 存储所有数据的列表\n",
    "train_data = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line.strip()))\n",
    "\n",
    "# load all training data tokens\n",
    "file_path = '/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/tokenized_code_tokens_train.jsonl'\n",
    "code_tokens_strs = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        code_tokens_strs.append(json.loads(line.strip()))\n",
    "\n",
    "nl_file_path = '/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/tokenized_comment_tokens_train.jsonl'\n",
    "nl_tokens_strs = []\n",
    "with open(nl_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        nl_tokens_strs.append(json.loads(line.strip()))\n",
    "\n",
    "# 现在 code_tokens_strs 变量中包含了从 JSON 文件读取的数据\n",
    "print(\"len(code_tokens_strs)\", len(code_tokens_strs))  # 可以查看加载的数据\n",
    "print(\"len(nl_tokens_strs)\", len(nl_tokens_strs))  # 可以查看加载的数据\n",
    "\n",
    "# Further tokenize code tokens and docstring tokens\n",
    "# Process all training data with token mapping\n",
    "tokens_output_path = \"/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/code_tokens_further.jsonl\"\n",
    "maps_output_path = \"/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/code_token_maps.jsonl\"\n",
    "\n",
    "# Check if both files exist\n",
    "if os.path.exists(tokens_output_path) and os.path.exists(maps_output_path):\n",
    "    print(\"Found existing tokenized files, loading them...\")\n",
    "    code_tokens_further_data = []\n",
    "    code_token_maps = []\n",
    "    \n",
    "    # Load existing tokens\n",
    "    with open(tokens_output_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            json_obj = json.loads(line)\n",
    "            code_tokens_further_data.append(json_obj['code_tokens_further'])\n",
    "            \n",
    "    # Load existing maps\n",
    "    with open(maps_output_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            json_obj = json.loads(line)\n",
    "            code_token_maps.append(json_obj['code_token_map'])\n",
    "            \n",
    "else:\n",
    "    print(\"Tokenized files not found, generating them...\")\n",
    "    code_tokens_further_data = []\n",
    "    code_token_maps = []\n",
    "\n",
    "    # Process whole data\n",
    "    for data in train_data:\n",
    "        code_tokens = data['code_tokens']\n",
    "        further_tokens, token_mapping = further_tokenize(code_tokens)\n",
    "        code_tokens_further_data.append(further_tokens)\n",
    "        code_token_maps.append(token_mapping)\n",
    "\n",
    "    # Save further tokens\n",
    "    with open(tokens_output_path, 'w', encoding='utf-8') as f:\n",
    "        for tokens in code_tokens_further_data:\n",
    "            json_obj = {'code_tokens_further': tokens}\n",
    "            f.write(json.dumps(json_obj) + '\\n')\n",
    "            \n",
    "\n",
    "    # Save token mappings \n",
    "    with open(maps_output_path, 'w', encoding='utf-8') as f:\n",
    "        for token_map in code_token_maps:\n",
    "            json_obj = {'code_token_map': token_map}\n",
    "            f.write(json.dumps(json_obj) + '\\n')\n",
    "\n",
    "# load already auto labeled info\n",
    "input_path = \"/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/sorted_labelling_sample_api_teacher.jsonl\"\n",
    "idx_list = []\n",
    "match_list = []\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip().rstrip(',')  # 去除行末的逗号\n",
    "        json_obj = json.loads(line)\n",
    "        idx_list.append(json_obj['idx'])\n",
    "        match_list.append(json_obj['match'])\n",
    "\n",
    "print(\"len(idx_list)\", len(idx_list)) \n",
    "\n",
    "\n",
    "# load map data\n",
    "with open(maps_output_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        code_token_maps.append(json_obj['code_token_map'])\n",
    "\n",
    "# load tokenize id data\n",
    "with open('/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/tokenized_id_train.json', 'r') as f:\n",
    "    tokenized_id_data = json.load(f)\n",
    "print(\"len(tokenized_id_data)\", len(tokenized_id_data))     \n",
    "\n",
    "\n",
    "# Load grammar patterns\n",
    "with open('/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/core_grammar_stats.json', 'r') as f:\n",
    "    grammar_stats = json.load(f)\n",
    "    sentence_patterns = grammar_stats['sentence_patterns']\n",
    "\n",
    "\n",
    "ast_vectors = np.load('/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/scaled_ast_vectors.npy')\n",
    "\n",
    "# Load or generate student-teacher pairs based on similarity\n",
    "student_teacher_pairs_file = \"/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/student_teachers_pairs.jsonl\"\n",
    "\n",
    "if os.path.exists(student_teacher_pairs_file):\n",
    "    student_teacher_pairs = []\n",
    "    with open(student_teacher_pairs_file, 'r') as f:\n",
    "        for line in f:\n",
    "            student_teacher_pairs.append(json.loads(line))\n",
    "    print(f\"Loaded {len(student_teacher_pairs)} existing student-teacher pairs\")\n",
    "\n",
    "else:\n",
    "    # Process each training example\n",
    "    student_teacher_pairs = []\n",
    "    for student_idx in range(len(ast_vectors)):\n",
    "        if student_idx in idx_list:\n",
    "            continue\n",
    "            \n",
    "        student_pattern = sentence_patterns[str(student_idx)]\n",
    "        student_vector = ast_vectors[student_idx]\n",
    "        \n",
    "        # Find teachers with same pattern\n",
    "        similarities = []\n",
    "        for teacher_idx in idx_list:\n",
    "            teacher_pattern = sentence_patterns[str(teacher_idx)]\n",
    "            \n",
    "            # Only consider teachers with same pattern\n",
    "            if teacher_pattern != student_pattern:\n",
    "                continue\n",
    "                \n",
    "            teacher_vector = ast_vectors[teacher_idx]\n",
    "            \n",
    "            # Calculate cosine similarity between AST vectors\n",
    "            similarity = np.dot(student_vector, teacher_vector) / (np.linalg.norm(student_vector) * np.linalg.norm(teacher_vector))\n",
    "            \n",
    "            similarities.append({\n",
    "                'teacher_idx': teacher_idx,\n",
    "                'confidence': similarity\n",
    "            })\n",
    "        \n",
    "        # Only proceed if we found at least 3 teachers\n",
    "        if len(similarities) >= 3:\n",
    "            # Sort by similarity and take top 3\n",
    "            similarities.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "            top_3_teachers = similarities[:3]\n",
    "            \n",
    "            pair = {\n",
    "                'student_idx': student_idx,\n",
    "                'teachers': top_3_teachers\n",
    "            }\n",
    "            # Write each pair to jsonl file\n",
    "            with open(student_teacher_pairs_file, 'a') as f:\n",
    "                f.write(json.dumps(pair) + '\\n')\n",
    "            student_teacher_pairs.append(pair)\n",
    "\n",
    "    print(f\"Generated and saved {len(student_teacher_pairs)} student-teacher pairs based on token similarity\")\n",
    "\n",
    "\n",
    "# Process all training data and save results\n",
    "print(\"Processing training data with unique symbols/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling.\")\n",
    "\n",
    "# Save processed docstring tokens to file\n",
    "docstring_tokens_unique_file = \"/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/docstring_tokens_unique.jsonl\"\n",
    "if not os.path.exists(docstring_tokens_unique_file):\n",
    "    with open(docstring_tokens_unique_file, 'w') as f:\n",
    "        for data in train_data:\n",
    "            data['docstring_tokens_unique'] = add_unique_symbols(data['docstring_tokens'], is_comment=True)\n",
    "            f.write(json.dumps({'docstring_tokens_unique': data['docstring_tokens_unique']}) + '\\n')\n",
    "else:\n",
    "    with open(docstring_tokens_unique_file, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            data = json.loads(line)\n",
    "            train_data[i]['docstring_tokens_unique'] = data['docstring_tokens_unique']\n",
    "\n",
    "# Save processed code tokens to file  \n",
    "code_tokens_unique_file = \"/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/code_tokens_unique.jsonl\"\n",
    "if not os.path.exists(code_tokens_unique_file):\n",
    "    with open(code_tokens_unique_file, 'w') as f:\n",
    "        for tokens in code_tokens_further_data:\n",
    "            unique_tokens = add_unique_symbols(tokens, is_comment=False)\n",
    "            f.write(json.dumps({'code_tokens_unique': unique_tokens}) + '\\n')\n",
    "else:\n",
    "    code_tokens_further_data_unique = []\n",
    "    with open(code_tokens_unique_file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            code_tokens_further_data_unique.append(data['code_tokens_unique'])\n",
    "\n",
    "print(\"Finished loading/processing unique symbols for tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 1868 samples that meet confidence thresholds\n",
      "Mean confidence threshold: 0.58\n",
      "Lowest confidence threshold: 0.4\n",
      "After filtering already labeled examples, 1868 samples remain\n",
      "\n",
      "=== Processing auto_label_ind: 131558 ===\n",
      "Teacher 0 idx: 231901, confidence: 0.7953409945552158\n",
      "Teacher 1 idx: 248329, confidence: 0.5380637718215854\n",
      "Teacher 2 idx: 48564, confidence: 0.42466903699517833\n",
      "Student idx: 194009\n",
      "Student tokens: \n",
      "    Here are the tokens you need to process:\n",
      "\n",
      "    Comment Tokens Index and Comment Tokens String:\n",
      "    ['Creates', 'a', 'new', 'local', 'user', 'and', 'assigns', 'admin', 'role']\n",
      "\n",
      "    Code Tokens Index and Code Tokens String:\n",
      "    ['def', 'create', '_', 'admin■1', '(■1', 'c■1', 'tx■1', ')■1', ':■1', 'try', ':■2', 'admin■2', '=■1', '_create_user', '(■2', 'c■2', 'tx■2', ')■2', 'admin■3', '.■1', 'roles', '.■2', 'append', '(■3', \"'■1\", 'admin■4', \"'■2\", ')■3', 'admin■5', '.■3', 'save', '(■4', ')■4', 'log■1', '(■5', 'Done', ')■5', 'except', 'Key', 'Error', ':■3', 'log■2', '(■6', \"'■3\", 'User', 'already', 'exists', \"'■4\", ',', 'lv', 'l', '=■2', 'warn', ')■6']\n",
      "    \n",
      "Response content:\n",
      "{\"alignments\":[{\"code_token\":[\"create\",\"_create_user\"],\"comment_token\":[\"Creates\"]},{\"code_token\":[\"local\",\"user\"],\"comment_token\":[\"new\",\"local\",\"user\"]},{\"code_token\":[\"admin■4\",\"roles\",\".■2\",\"append\",\"admin■5\",\".■3\",\"save\"],\"comment_token\":[\"assigns\",\"admin\",\"role\"]}]}\n",
      "Successfully parsed JSON response\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: Creates, Cleaned token: Creates\n",
      "Match found from index 0 to 1\n",
      "Final comment matches: [0, 1]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token create at index 1\n",
      "Original key 1 was converted to integer 1\n",
      "Original token at index 1 was tokenized to range [1, 4]\n",
      "Range tokens: ['Ġcreate', '_', 'admin']\n",
      "Added minimal indices [1] for combined token create\n",
      "Found token _create_user at index 13\n",
      "Original key 10 was converted to integer 10\n",
      "Original token at index 10 was tokenized to range [13, 17]\n",
      "Range tokens: ['Ġ_', 'create', '_', 'user']\n",
      "Added minimal indices [16, 13, 14, 15] for combined token _create_user\n",
      "Final code matches: [1, 16, 13, 14, 15]\n",
      "Added to final results: [[0, 1], [1, 16, 13, 14, 15]]\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: new, Cleaned token: new\n",
      "Match found from index 3 to 3\n",
      "Original token: local, Cleaned token: local\n",
      "Match found from index 4 to 4\n",
      "Original token: user, Cleaned token: user\n",
      "Match found from index 5 to 5\n",
      "Final comment matches: [3, 4, 5]\n",
      "\n",
      "Processing code tokens:\n",
      "Could not find token local in unique tokens - hallucination detected\n",
      "Hallucination detected - skipping all alignments\n",
      "No final results for auto_label_ind: 131558\n",
      "\n",
      "=== Processing auto_label_ind: 97685 ===\n",
      "Teacher 0 idx: 127022, confidence: 0.9371408833693649\n",
      "Teacher 1 idx: 71348, confidence: 0.506313350772398\n",
      "Teacher 2 idx: 102448, confidence: 0.4707284391744218\n",
      "Student idx: 143572\n",
      "Student tokens: \n",
      "    Here are the tokens you need to process:\n",
      "\n",
      "    Comment Tokens Index and Comment Tokens String:\n",
      "    ['Actions', 'to', 'be', 'done', 'when', 'an▲1', 'analysis', 'is', 'added', 'in', 'an▲2', 'Analysis', 'Request']\n",
      "\n",
      "    Code Tokens Index and Code Tokens String:\n",
      "    ['def', 'Object', 'Initialized', 'Event', 'Handler', '(■1', 'analysis■1', ',■1', 'event', ')■1', ':', 'wf■1', '.■1', 'doActionFor■1', '(■2', 'analysis■2', ',■2', 'initialize', ')■2', 'request■1', '=■1', 'analysis■3', '.■2', 'getRequest', '(■3', ')■3', 'wf■2', '.■3', 'doActionFor■2', '(■4', 'request■2', ',■3', 'rollback', '_■1', 'to', '_■2', 'receive', ')■4', 'analysis■4', '.■4', 'reindexObject', '(■5', 'i', 'dx', 's', '=■2', 'get', 'Service', 'U', 'ID', ')■5', 'return']\n",
      "    \n",
      "Response content:\n",
      "{\"alignments\":[{\"code_token\":[\"doActionFor■1\",\"doActionFor■2\",\"reindexObject\"],\"comment_token\":[\"Actions\",\"done\",\"added\"]},{\"code_token\":[\"analysis■1\",\"analysis■2\",\"analysis■3\",\"analysis■4\"],\"comment_token\":[\"analysis\",\"Analysis\"]},{\"code_token\":[\"event\",\"request■1\",\"request■2\"],\"comment_token\":[\"in\",\"Request\"]}]}\n",
      "Successfully parsed JSON response\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: Actions, Cleaned token: Actions\n",
      "Match found from index 0 to 1\n",
      "Original token: done, Cleaned token: done\n",
      "Match found from index 4 to 4\n",
      "Original token: added, Cleaned token: added\n",
      "Match found from index 9 to 9\n",
      "Final comment matches: [0, 1, 4, 9]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token doActionFor■1 at index 13\n",
      "Original key 10 was converted to integer 10\n",
      "Original token at index 10 was tokenized to range [14, 17]\n",
      "Range tokens: ['Ġdo', 'Action', 'For']\n",
      "Added minimal indices [16, 14, 15] for combined token doActionFor\n",
      "Found token doActionFor■2 at index 28\n",
      "Original key 25 was converted to integer 25\n",
      "Original token at index 25 was tokenized to range [36, 39]\n",
      "Range tokens: ['Ġdo', 'Action', 'For']\n",
      "Added minimal indices [36, 37, 38] for combined token doActionFor\n",
      "Found token reindexObject at index 40\n",
      "Original key 33 was converted to integer 33\n",
      "Original token at index 33 was tokenized to range [54, 57]\n",
      "Range tokens: ['Ġre', 'index', 'Object']\n",
      "Added minimal indices [56, 54, 55] for combined token reindexObject\n",
      "Final code matches: [16, 14, 15, 36, 37, 38, 56, 54, 55]\n",
      "Added to final results: [[0, 1, 4, 9], [16, 14, 15, 36, 37, 38, 56, 54, 55]]\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: analysis, Cleaned token: analysis\n",
      "Match found from index 7 to 7\n",
      "Original token: Analysis, Cleaned token: Analysis\n",
      "Match found from index 12 to 12\n",
      "Final comment matches: [7, 12]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token analysis■1 at index 6\n",
      "Original key 3 was converted to integer 3\n",
      "Original token at index 3 was tokenized to range [6, 7]\n",
      "Range tokens: ['Ġanalysis']\n",
      "Added minimal indices [6] for combined token analysis\n",
      "Found token analysis■2 at index 15\n",
      "Original key 12 was converted to integer 12\n",
      "Original token at index 12 was tokenized to range [18, 19]\n",
      "Range tokens: ['Ġanalysis']\n",
      "Added minimal indices [18] for combined token analysis\n",
      "Found token analysis■3 at index 21\n",
      "Original key 18 was converted to integer 18\n",
      "Original token at index 18 was tokenized to range [27, 28]\n",
      "Range tokens: ['Ġanalysis']\n",
      "Added minimal indices [27] for combined token analysis\n",
      "Found token analysis■4 at index 38\n",
      "Original key 31 was converted to integer 31\n",
      "Original token at index 31 was tokenized to range [52, 53]\n",
      "Range tokens: ['Ġanalysis']\n",
      "Added minimal indices [52] for combined token analysis\n",
      "Final code matches: [6, 18, 27, 52]\n",
      "Added to final results: [[7, 12], [6, 18, 27, 52]]\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: in, Cleaned token: in\n",
      "Match found from index 10 to 10\n",
      "Original token: Request, Cleaned token: Request\n",
      "Match found from index 13 to 13\n",
      "Final comment matches: [10, 13]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token event at index 8\n",
      "Original key 5 was converted to integer 5\n",
      "Original token at index 5 was tokenized to range [8, 9]\n",
      "Range tokens: ['Ġevent']\n",
      "Added minimal indices [8] for combined token event\n",
      "Found token request■1 at index 19\n",
      "Original key 16 was converted to integer 16\n",
      "Original token at index 16 was tokenized to range [25, 26]\n",
      "Range tokens: ['Ġrequest']\n",
      "Added minimal indices [25] for combined token request\n",
      "Found token request■2 at index 30\n",
      "Original key 27 was converted to integer 27\n",
      "Original token at index 27 was tokenized to range [40, 41]\n",
      "Range tokens: ['Ġrequest']\n",
      "Added minimal indices [40] for combined token request\n",
      "Final code matches: [8, 25, 40]\n",
      "Added to final results: [[10, 13], [8, 25, 40]]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [0, 1, 4, 4, 9, 9]\n",
      "Code intervals: [14, 16, 36, 38, 54, 56]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [7, 7, 12, 12]\n",
      "Code intervals: [6, 6, 18, 18, 27, 27, 52, 52]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [10, 10, 13, 13]\n",
      "Code intervals: [8, 8, 25, 25, 40, 40]\n",
      "Final results: [[[0, 1, 4, 4, 9, 9], [14, 16, 36, 38, 54, 56]], [[7, 7, 12, 12], [6, 6, 18, 18, 27, 27, 52, 52]], [[10, 10, 13, 13], [8, 8, 25, 25, 40, 40]]]\n",
      "Successfully wrote new entry to JSONL file\n",
      "\n",
      "=== Processing auto_label_ind: 59365 ===\n",
      "Teacher 0 idx: 248329, confidence: 0.9525314551545626\n",
      "Teacher 1 idx: 158284, confidence: 0.4622332450356434\n",
      "Teacher 2 idx: 231901, confidence: 0.4127123451905995\n",
      "Student idx: 87266\n",
      "Student tokens: \n",
      "    Here are the tokens you need to process:\n",
      "\n",
      "    Comment Tokens Index and Comment Tokens String:\n",
      "    ['Parse', 'an', 'xml', '.▲1', 'dom', 'Node', 'object▲1', 'representing', 'a', 'target', 'execution', 'context', 'into', 'this', 'object▲2', '.▲2']\n",
      "\n",
      "    Code Tokens Index and Code Tokens String:\n",
      "    ['def', 'parse■1', '_■1', 'xml■1', '_■2', 'node■1', '(■1', 'self■1', ',■1', 'node■2', ')■1', ':■1', 'super', '(■2', 'Target', 'Execution', 'Context', ',■2', 'self■2', ')■2', '.■1', 'parse■2', '_■3', 'xml■2', '_■4', 'node■3', '(■3', 'node■4', ')■3', 'if', 'node■5', '.■2', 'hasAttributeNS', '(■4', 'RTS■1', '_■5', 'NS■1', ',■3', \"'■1\", 'id■1', \"'■2\", ')■4', ':■2', 'self■3', '.■3', 'id■2', '=■1', 'node■6', '.■4', 'getAttributeNS', '(■5', 'RTS■2', '_■6', 'NS■2', ',■4', \"'■3\", 'id■3', \"'■4\", ')■5', 'else', ':■3', 'self■4', '.■5', 'id■4', '=■2', \"'■5\", \"'■6\", 'return', 'self■5']\n",
      "    \n",
      "Response content:\n",
      "{\"alignments\":[{\"code_token\":[\"parse■1\",\"parse■2\"],\"comment_token\":[\"Parse\"]},{\"code_token\":[\"xml■1\",\"xml■2\"],\"comment_token\":[\"xml\"]},{\"code_token\":[\"node■1\",\"node■2\",\"node■3\",\"node■4\",\"node■5\",\"node■6\"],\"comment_token\":[\"Node\",\"object▲1\"]},{\"code_token\":[\"Target\",\"Execution\",\"Context\"],\"comment_token\":[\"execution\",\"context\",\"object▲2\"]}]}\n",
      "Successfully parsed JSON response\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: Parse, Cleaned token: Parse\n",
      "Match found from index 0 to 1\n",
      "Final comment matches: [0, 1]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token parse■1 at index 1\n",
      "Original key 1 was converted to integer 1\n",
      "Original token at index 1 was tokenized to range [1, 6]\n",
      "Range tokens: ['Ġparse', '_', 'xml', '_', 'node']\n",
      "Added minimal indices [1] for combined token parse\n",
      "Found token parse■2 at index 21\n",
      "Original key 15 was converted to integer 15\n",
      "Original token at index 15 was tokenized to range [22, 27]\n",
      "Range tokens: ['Ġparse', '_', 'xml', '_', 'node']\n",
      "Added minimal indices [22] for combined token parse\n",
      "Final code matches: [1, 22]\n",
      "Added to final results: [[0, 1], [1, 22]]\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: xml, Cleaned token: xml\n",
      "Match found from index 3 to 3\n",
      "Final comment matches: [3]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token xml■1 at index 3\n",
      "Original key 1 was converted to integer 1\n",
      "Original token at index 1 was tokenized to range [1, 6]\n",
      "Range tokens: ['Ġparse', '_', 'xml', '_', 'node']\n",
      "Added minimal indices [3] for combined token xml\n",
      "Found token xml■2 at index 23\n",
      "Original key 15 was converted to integer 15\n",
      "Original token at index 15 was tokenized to range [22, 27]\n",
      "Range tokens: ['Ġparse', '_', 'xml', '_', 'node']\n",
      "Added minimal indices [24] for combined token xml\n",
      "Final code matches: [3, 24]\n",
      "Added to final results: [[3], [3, 24]]\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: Node, Cleaned token: Node\n",
      "Match found from index 6 to 6\n",
      "Original token: object▲1, Cleaned token: object\n",
      "Match found from index 7 to 7\n",
      "Final comment matches: [6, 7]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token node■1 at index 5\n",
      "Original key 1 was converted to integer 1\n",
      "Original token at index 1 was tokenized to range [1, 6]\n",
      "Range tokens: ['Ġparse', '_', 'xml', '_', 'node']\n",
      "Added minimal indices [5] for combined token node\n",
      "Found token node■2 at index 9\n",
      "Original key 5 was converted to integer 5\n",
      "Original token at index 5 was tokenized to range [9, 10]\n",
      "Range tokens: ['Ġnode']\n",
      "Added minimal indices [9] for combined token node\n",
      "Found token node■3 at index 25\n",
      "Original key 15 was converted to integer 15\n",
      "Original token at index 15 was tokenized to range [22, 27]\n",
      "Range tokens: ['Ġparse', '_', 'xml', '_', 'node']\n",
      "Added minimal indices [26] for combined token node\n",
      "Found token node■4 at index 27\n",
      "Original key 17 was converted to integer 17\n",
      "Original token at index 17 was tokenized to range [28, 29]\n",
      "Range tokens: ['Ġnode']\n",
      "Added minimal indices [28] for combined token node\n",
      "Found token node■5 at index 30\n",
      "Original key 20 was converted to integer 20\n",
      "Original token at index 20 was tokenized to range [31, 32]\n",
      "Range tokens: ['Ġnode']\n",
      "Added minimal indices [31] for combined token node\n",
      "Found token node■6 at index 47\n",
      "Original key 33 was converted to integer 33\n",
      "Original token at index 33 was tokenized to range [51, 52]\n",
      "Range tokens: ['Ġnode']\n",
      "Added minimal indices [51] for combined token node\n",
      "Final code matches: [5, 9, 26, 28, 31, 51]\n",
      "Added to final results: [[6, 7], [5, 9, 26, 28, 31, 51]]\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: execution, Cleaned token: execution\n",
      "Match found from index 11 to 11\n",
      "Original token: context, Cleaned token: context\n",
      "Match found from index 12 to 12\n",
      "Original token: object▲2, Cleaned token: object\n",
      "Match found from index 15 to 15\n",
      "Final comment matches: [11, 12, 15]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token Target at index 14\n",
      "Original key 10 was converted to integer 10\n",
      "Original token at index 10 was tokenized to range [14, 18]\n",
      "Range tokens: ['ĠTarget', 'Exec', 'ution', 'Context']\n",
      "Added minimal indices [14] for combined token Target\n",
      "Found token Execution at index 15\n",
      "Original key 10 was converted to integer 10\n",
      "Original token at index 10 was tokenized to range [14, 18]\n",
      "Range tokens: ['ĠTarget', 'Exec', 'ution', 'Context']\n",
      "Added minimal indices [16, 15] for combined token Execution\n",
      "Found token Context at index 16\n",
      "Original key 10 was converted to integer 10\n",
      "Original token at index 10 was tokenized to range [14, 18]\n",
      "Range tokens: ['ĠTarget', 'Exec', 'ution', 'Context']\n",
      "Added minimal indices [17] for combined token Context\n",
      "Final code matches: [14, 16, 15, 17]\n",
      "Added to final results: [[11, 12, 15], [14, 16, 15, 17]]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [0, 1]\n",
      "Code intervals: [1, 1, 22, 22]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [3, 3]\n",
      "Code intervals: [3, 3, 24, 24]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [6, 7]\n",
      "Code intervals: [5, 5, 9, 9, 26, 26, 28, 28, 31, 31, 51, 51]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [11, 12, 15, 15]\n",
      "Code intervals: [14, 17]\n",
      "Final results: [[[0, 1], [1, 1, 22, 22]], [[3, 3], [3, 3, 24, 24]], [[6, 7], [5, 5, 9, 9, 26, 26, 28, 28, 31, 31, 51, 51]], [[11, 12, 15, 15], [14, 17]]]\n",
      "Successfully wrote new entry to JSONL file\n",
      "\n",
      "=== Processing auto_label_ind: 139153 ===\n",
      "Teacher 0 idx: 231901, confidence: 0.8024179041805872\n",
      "Teacher 1 idx: 248329, confidence: 0.5191082333515555\n",
      "Teacher 2 idx: 48564, confidence: 0.4313482587140912\n",
      "Student idx: 205587\n",
      "Student tokens: \n",
      "    Here are the tokens you need to process:\n",
      "\n",
      "    Comment Tokens Index and Comment Tokens String:\n",
      "    ['GET', 'method', 'implementation', 'for', 'a', 'note', 'detail']\n",
      "\n",
      "    Code Tokens Index and Code Tokens String:\n",
      "    ['def', 'retrieve', '(■1', 'self', ',■1', 'request', ',■2', 'project', ',■3', 'pk■1', '=■1', 'None', ')■1', ':■1', 'try', ':■2', 'serialize', 'r', '=■2', 'JobNoteSerializer', '(■2', 'JobNote■1', '.■1', 'objects', '.■2', 'get', '(■3', 'id', '=■3', 'pk■2', ')■2', ')■3', 'return■1', 'Response■1', '(■4', 'serializer', '.■3', 'data', ')■4', 'except', 'JobNote■2', '.■4', 'DoesNotExist', ':■3', 'return■2', 'Response■2', '(■5', '\"No note with id: {0}\"', '.■5', 'format', '(■6', 'pk■3', ')■5', ',■4', 'status', '=■4', 'HTTP', '_■1', '404', '_■2', 'NOT', '_■3', 'FOUND', ')■6']\n",
      "    \n",
      "Response content:\n",
      "{\"alignments\":[{\"code_token\":[\"retrieve\",\"JobNote■1\"],\"comment_token\":[\"GET\",\"method\"]},{\"code_token\":[\"JobNoteSerializer\",\"serialize\",\"serializer\"],\"comment_token\":[\"implementation\"]},{\"code_token\":[\"id\",\"pk■1\",\"pk■2\"],\"comment_token\":[\"note\",\"detail\"]}]}\n",
      "Successfully parsed JSON response\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: GET, Cleaned token: GET\n",
      "Match found from index 0 to 0\n",
      "Original token: method, Cleaned token: method\n",
      "Match found from index 1 to 1\n",
      "Final comment matches: [0, 1]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token retrieve at index 1\n",
      "Original key 1 was converted to integer 1\n",
      "Original token at index 1 was tokenized to range [1, 2]\n",
      "Range tokens: ['Ġretrieve']\n",
      "Added minimal indices [1] for combined token retrieve\n",
      "Found token JobNote■1 at index 21\n",
      "Original key 20 was converted to integer 20\n",
      "Original token at index 20 was tokenized to range [25, 27]\n",
      "Range tokens: ['ĠJob', 'Note']\n",
      "Added minimal indices [25, 26] for combined token JobNote\n",
      "Final code matches: [1, 25, 26]\n",
      "Added to final results: [[0, 1], [1, 25, 26]]\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: implementation, Cleaned token: implementation\n",
      "Match found from index 2 to 2\n",
      "Final comment matches: [2]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token JobNoteSerializer at index 19\n",
      "Original key 18 was converted to integer 18\n",
      "Original token at index 18 was tokenized to range [20, 24]\n",
      "Range tokens: ['ĠJob', 'Note', 'Serial', 'izer']\n",
      "Added minimal indices [20, 21, 22, 23] for combined token JobNoteSerializer\n",
      "Found token serialize at index 16\n",
      "Original key 16 was converted to integer 16\n",
      "Original token at index 16 was tokenized to range [17, 19]\n",
      "Range tokens: ['Ġserial', 'izer']\n",
      "No individual matches found - matching available indices [17, 18] for combined token serializer to serialize\n",
      "Found token serializer at index 35\n",
      "Original key 34 was converted to integer 34\n",
      "Original token at index 34 was tokenized to range [41, 43]\n",
      "Range tokens: ['Ġserial', 'izer']\n",
      "Added minimal indices [41, 42] for combined token serializer\n",
      "Final code matches: [20, 21, 22, 23, 17, 18, 41, 42]\n",
      "Added to final results: [[2], [20, 21, 22, 23, 17, 18, 41, 42]]\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: note, Cleaned token: note\n",
      "Match found from index 5 to 5\n",
      "Original token: detail, Cleaned token: detail\n",
      "Match found from index 6 to 6\n",
      "Final comment matches: [5, 6]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token id at index 27\n",
      "Original key 26 was converted to integer 26\n",
      "Original token at index 26 was tokenized to range [32, 33]\n",
      "Range tokens: ['Ġid']\n",
      "Added minimal indices [32] for combined token id\n",
      "Found token pk■1 at index 9\n",
      "Original key 9 was converted to integer 9\n",
      "Original token at index 9 was tokenized to range [9, 11]\n",
      "Range tokens: ['Ġp', 'k']\n",
      "Added minimal indices [9, 10] for combined token pk\n",
      "Found token pk■2 at index 29\n",
      "Original key 28 was converted to integer 28\n",
      "Original token at index 28 was tokenized to range [34, 36]\n",
      "Range tokens: ['Ġp', 'k']\n",
      "Added minimal indices [34, 35] for combined token pk\n",
      "Final code matches: [32, 9, 10, 34, 35]\n",
      "Added to final results: [[5, 6], [32, 9, 10, 34, 35]]\n",
      "Post-validation: Added identical unmatched token at index 62: id to round 2\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [0, 1]\n",
      "Code intervals: [1, 1, 25, 26]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [2, 2]\n",
      "Code intervals: [17, 18, 20, 23, 41, 42]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [5, 6]\n",
      "Code intervals: [9, 10, 32, 32, 34, 35, 62, 62]\n",
      "Final results: [[[0, 1], [1, 1, 25, 26]], [[2, 2], [17, 18, 20, 23, 41, 42]], [[5, 6], [9, 10, 32, 32, 34, 35, 62, 62]]]\n",
      "Successfully wrote new entry to JSONL file\n",
      "\n",
      "=== Processing auto_label_ind: 40761 ===\n",
      "Teacher 0 idx: 248329, confidence: 0.9397992765511735\n",
      "Teacher 1 idx: 231901, confidence: 0.43462801176896937\n",
      "Teacher 2 idx: 158284, confidence: 0.4027289774538852\n",
      "Student idx: 60000\n",
      "Student tokens: \n",
      "    Here are the tokens you need to process:\n",
      "\n",
      "    Comment Tokens Index and Comment Tokens String:\n",
      "    ['Creates', 'a▲1', 'DNA', 'duplex', 'from', 'a▲2', 'nucleotide', 'sequence', '.']\n",
      "\n",
      "    Code Tokens Index and Code Tokens String:\n",
      "    ['def', 'from', '_■1', 'sequence■1', '(■1', 'cls■1', ',■1', 'sequence■2', ',■2', 'ph■1', 'os■1', '_■2', '3■1', '_■3', 'prime■1', '=■1', 'False', ')■1', ':', 'strand■1', '1■1', '=■2', 'NucleicAcidStrand', '(■2', 'sequence■3', ',■3', 'ph■2', 'os■2', '_■4', '3■2', '_■5', 'prime■2', '=■3', 'ph■3', 'os■3', '_■6', '3■3', '_■7', 'prime■3', ')■2', 'duplex■1', '=■4', 'cls■2', '(■3', 'strand■2', '1■2', ')■3', 'return', 'duplex■2']\n",
      "    \n",
      "Response content:\n",
      "{\"alignments\":[{\"comment_token\":[\"Creates\"],\"code_token\":[\"def\",\"cls■2\"]},{\"comment_token\":[\"DNA\",\"duplex\"],\"code_token\":[\"duplex■1\",\"duplex■2\"]},{\"comment_token\":[\"nucleotide\",\"sequence\"],\"code_token\":[\"sequence■1\",\"sequence■2\",\"sequence■3\",\"NucleicAcidStrand\"]}]}\n",
      "Successfully parsed JSON response\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: Creates, Cleaned token: Creates\n",
      "Match found from index 0 to 1\n",
      "Final comment matches: [0, 1]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token def at index 0\n",
      "Original key 0 was converted to integer 0\n",
      "Original token at index 0 was tokenized to range [0, 1]\n",
      "Range tokens: ['def']\n",
      "Added minimal indices [0] for combined token def\n",
      "Found token cls■2 at index 42\n",
      "Original key 24 was converted to integer 24\n",
      "Original token at index 24 was tokenized to range [50, 52]\n",
      "Range tokens: ['Ġcl', 's']\n",
      "Added minimal indices [50, 51] for combined token cls\n",
      "Final code matches: [0, 50, 51]\n",
      "Added to final results: [[0, 1], [0, 50, 51]]\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: DNA, Cleaned token: DNA\n",
      "Match found from index 3 to 3\n",
      "Original token: duplex, Cleaned token: duplex\n",
      "Match found from index 4 to 5\n",
      "Final comment matches: [3, 4, 5]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token duplex■1 at index 40\n",
      "Original key 22 was converted to integer 22\n",
      "Original token at index 22 was tokenized to range [47, 49]\n",
      "Range tokens: ['Ġdup', 'lex']\n",
      "Added minimal indices [48, 47] for combined token duplex\n",
      "Found token duplex■2 at index 48\n",
      "Original key 29 was converted to integer 29\n",
      "Original token at index 29 was tokenized to range [57, 59]\n",
      "Range tokens: ['Ġdup', 'lex']\n",
      "Added minimal indices [57, 58] for combined token duplex\n",
      "Final code matches: [48, 47, 57, 58]\n",
      "Added to final results: [[3, 4, 5], [48, 47, 57, 58]]\n",
      "\n",
      "Processing comment tokens:\n",
      "Original token: nucleotide, Cleaned token: nucleotide\n",
      "Match found from index 8 to 9\n",
      "Original token: sequence, Cleaned token: sequence\n",
      "Match found from index 10 to 10\n",
      "Final comment matches: [8, 9, 10]\n",
      "\n",
      "Processing code tokens:\n",
      "Found token sequence■1 at index 3\n",
      "Original key 1 was converted to integer 1\n",
      "Original token at index 1 was tokenized to range [1, 4]\n",
      "Range tokens: ['Ġfrom', '_', 'sequence']\n",
      "Added minimal indices [3] for combined token sequence\n",
      "Found token sequence■2 at index 7\n",
      "Original key 5 was converted to integer 5\n",
      "Original token at index 5 was tokenized to range [8, 9]\n",
      "Range tokens: ['Ġsequence']\n",
      "Added minimal indices [8] for combined token sequence\n",
      "Found token sequence■3 at index 24\n",
      "Original key 16 was converted to integer 16\n",
      "Original token at index 16 was tokenized to range [31, 32]\n",
      "Range tokens: ['Ġsequence']\n",
      "Added minimal indices [31] for combined token sequence\n",
      "Found token NucleicAcidStrand at index 22\n",
      "Original key 14 was converted to integer 14\n",
      "Original token at index 14 was tokenized to range [23, 30]\n",
      "Range tokens: ['ĠN', 'ucle', 'ic', 'Ac', 'id', 'Str', 'and']\n",
      "Added minimal indices [23, 24, 25, 26, 27, 28, 29] for combined token NucleicAcidStrand\n",
      "Final code matches: [3, 8, 31, 23, 24, 25, 26, 27, 28, 29]\n",
      "Added to final results: [[8, 9, 10], [3, 8, 31, 23, 24, 25, 26, 27, 28, 29]]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [0, 1]\n",
      "Code intervals: [0, 0, 50, 51]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [3, 5]\n",
      "Code intervals: [47, 48, 57, 58]\n",
      "\n",
      "Converted to intervals:\n",
      "Comment intervals: [8, 10]\n",
      "Code intervals: [3, 3, 8, 8, 23, 29, 31, 31]\n",
      "Final results: [[[0, 1], [0, 0, 50, 51]], [[3, 5], [47, 48, 57, 58]], [[8, 10], [3, 3, 8, 8, 23, 29, 31, 31]]]\n",
      "Successfully wrote new entry to JSONL file\n"
     ]
    }
   ],
   "source": [
    "# Set confidence thresholds\n",
    "MEAN_CONF_THRESHOLD = 0.58  # Minimum required mean confidence\n",
    "LOWEST_CONF_THRESHOLD = 0.4  # Minimum required lowest confidence\n",
    "\n",
    "# Filter student-teacher pairs based on confidence thresholds\n",
    "auto_label_indices = []\n",
    "for idx, pair in enumerate(student_teacher_pairs):\n",
    "    # Get lowest confidence among teachers\n",
    "    lowest_conf = min(teacher['confidence'] for teacher in pair['teachers'])\n",
    "    \n",
    "    # Only keep pairs that meet both thresholds\n",
    "    mean_conf = sum(teacher['confidence'] for teacher in pair['teachers']) / len(pair['teachers'])\n",
    "    if mean_conf >= MEAN_CONF_THRESHOLD and lowest_conf >= LOWEST_CONF_THRESHOLD:\n",
    "        auto_label_indices.append(idx)\n",
    "\n",
    "print(f\"Selected {len(auto_label_indices)} samples that meet confidence thresholds\")\n",
    "print(f\"Mean confidence threshold: {MEAN_CONF_THRESHOLD}\")\n",
    "print(f\"Lowest confidence threshold: {LOWEST_CONF_THRESHOLD}\")\n",
    "\n",
    "# Load previously labeled data to avoid duplicates\n",
    "labeled_indices = set()\n",
    "try:\n",
    "    with open('/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/sorted_labelling_sample_api_student_conf.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            labeled_indices.add(entry['idx'])\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing labeled data file found, starting fresh\")\n",
    "\n",
    "# Filter out already labeled examples and shuffle remaining indices\n",
    "auto_label_indices = [idx for idx in auto_label_indices \n",
    "                     if student_teacher_pairs[idx]['student_idx'] not in labeled_indices]\n",
    "random.shuffle(auto_label_indices)\n",
    "\n",
    "print(f\"After filtering already labeled examples, {len(auto_label_indices)} samples remain\")\n",
    "\n",
    "system_prompt = \"You are an expert at aligning tokens between comments and code. You can accurately identify the similarities and differences between tokens, and you are highly skilled at matching tokens based on their semantics and functionality. You are given input data consisting of comment tokens and code tokens, and your task is to align them by identifying concepts in the comments and matching them to corresponding code tokens. Use the example cases below and output your results in the specified format.\"\n",
    "\n",
    "# auto labelling \n",
    "# Only process first 10 examples for testing\n",
    "for auto_label_ind in auto_label_indices[:5]:\n",
    "    # Get student and teacher indices from the current pair\n",
    "    student_teacher_pair = student_teacher_pairs[auto_label_ind]\n",
    "    student_idx = student_teacher_pair['student_idx']\n",
    "    teachers = student_teacher_pair['teachers']\n",
    "\n",
    "    # Initialize list to store all teacher examples\n",
    "    teacher_examples = []\n",
    "\n",
    "    # Process each teacher\n",
    "    for teacher in teachers:\n",
    "        # Get the actual teacher index from the dictionary\n",
    "        teacher_idx = teacher['teacher_idx']\n",
    "        \n",
    "        # Get unique symbolized tokens for docstring and code\n",
    "        unique_docstring_tokens = train_data[teacher_idx]['docstring_tokens_unique']\n",
    "        unique_code_tokens = code_tokens_further_data_unique[teacher_idx]\n",
    "        \n",
    "        # Initialize list to store all alignments for this teacher\n",
    "        alignments = []\n",
    "\n",
    "        # Find the index in idx list that matches teacher_idx\n",
    "        teacher_ind = None\n",
    "        for i, entry in enumerate(idx_list):\n",
    "            if entry == teacher_idx:\n",
    "                teacher_ind = i\n",
    "                break\n",
    "                \n",
    "        if teacher_ind is None:\n",
    "            print(f\"Could not find teacher_idx {teacher_idx} in idx_list\")\n",
    "            continue\n",
    "\n",
    "        # For each match pair in the match list\n",
    "        for match_pair in match_list[teacher_ind]:\n",
    "            # Get comment and code match indices\n",
    "            comment_match = match_pair[0]\n",
    "            code_match = match_pair[1]\n",
    "            \n",
    "            # Initialize lists for current pair's tokens\n",
    "            current_comment_tokens = []\n",
    "            current_code_tokens = []\n",
    "            \n",
    "            # Extract matched comment tokens using indices\n",
    "            for i in range(0, len(comment_match), 2):\n",
    "                start, end = comment_match[i], comment_match[i+1]\n",
    "                tokens = nl_tokens_strs[teacher_idx][1:][start:end+1]\n",
    "                for token in tokens:\n",
    "                    clean_token = token.replace('Ġ', '')\n",
    "                    current_comment_tokens.append(clean_token)\n",
    "                \n",
    "            # Extract matched code tokens using indices  \n",
    "            for i in range(0, len(code_match), 2):\n",
    "                start, end = code_match[i], code_match[i+1]\n",
    "                tokens = code_tokens_strs[teacher_idx][1:][start:end+1]\n",
    "                for token in tokens:\n",
    "                    clean_token = token.replace('Ġ', '')\n",
    "                    current_code_tokens.append(clean_token)\n",
    "\n",
    "            # Map current pair's tokens to new token lists\n",
    "            docstring_tokens = train_data[teacher_idx]['docstring_tokens']\n",
    "            code_tokens = code_tokens_further_data[teacher_idx]\n",
    "            \n",
    "            # Create mappings for current pair\n",
    "            current_comment_map = {}\n",
    "            current_code_map = {}\n",
    "            \n",
    "            # Map comment tokens for current pair\n",
    "            doc_start_idx = 0\n",
    "            for i, matched_token in enumerate(current_comment_tokens):\n",
    "                for j in range(doc_start_idx, len(docstring_tokens)):\n",
    "                    if matched_token in docstring_tokens[j]:\n",
    "                        current_comment_map[f\"{i}_{matched_token}\"] = f\"{j}_{docstring_tokens[j]}\"\n",
    "                        doc_start_idx = j + 1\n",
    "                        break\n",
    "                \n",
    "            # Map code tokens for current pair\n",
    "            code_start_idx = 0\n",
    "            for i, matched_token in enumerate(current_code_tokens):\n",
    "                for j in range(code_start_idx, len(code_tokens)):\n",
    "                    if matched_token in code_tokens[j]:\n",
    "                        current_code_map[f\"{i}_{matched_token}\"] = f\"{j}_{code_tokens[j]}\"\n",
    "                        code_start_idx = j + 1\n",
    "                        break\n",
    "\n",
    "            # Extract indices from current mappings\n",
    "            comment_indices = [int(v.split('_')[0]) for v in current_comment_map.values()]\n",
    "            code_indices = [int(v.split('_')[0]) for v in current_code_map.values()]\n",
    "\n",
    "            # Get corresponding unique symbolized tokens\n",
    "            comment_unique_tokens = [unique_docstring_tokens[idx] for idx in comment_indices]\n",
    "            code_unique_tokens = [unique_code_tokens[idx] for idx in code_indices]\n",
    "\n",
    "            # Create alignment dict for current pair\n",
    "            alignment = {\n",
    "                \"comment_token\": comment_unique_tokens,\n",
    "                \"code_token\": code_unique_tokens\n",
    "            }\n",
    "            \n",
    "            # Add to alignments list\n",
    "            alignments.append(alignment)\n",
    "\n",
    "        # Create teacher example for current teacher\n",
    "        teacher_example = f\"\"\"\n",
    "        **Teacher Example {len(teacher_examples) + 1}:**\n",
    "        Comment Tokens Index and Comment Tokens String:\n",
    "        {unique_docstring_tokens}\n",
    "        Code Tokens Index and Code Tokens String:\n",
    "        {unique_code_tokens}\n",
    "        **Matching Output:**\n",
    "        {alignments}\n",
    "        \"\"\"\n",
    "        teacher_examples.append(teacher_example)\n",
    "\n",
    "    # Combine all teacher examples into one prompt\n",
    "    teacher_prompt = \"\\n\".join(teacher_examples)\n",
    "\n",
    "    # construct student input with indices\n",
    "    unique_student_docstring_tokens = train_data[student_idx]['docstring_tokens_unique']\n",
    "    unique_student_code_tokens = code_tokens_further_data_unique[student_idx]\n",
    "\n",
    "    student_docstring_tokens_raw = nl_tokens_strs[student_idx][1:]\n",
    "    student_code_tokens_raw = code_tokens_strs[student_idx][1:]\n",
    "\n",
    "    # construct student_prompt\n",
    "    student_tokens_part = f\"\"\"\n",
    "    Here are the tokens you need to process:\n",
    "\n",
    "    Comment Tokens Index and Comment Tokens String:\n",
    "    {unique_student_docstring_tokens}\n",
    "\n",
    "    Code Tokens Index and Code Tokens String:\n",
    "    {unique_student_code_tokens}\n",
    "    \"\"\"\n",
    "\n",
    "    alignment_format = \"\"\"\n",
    "    {\n",
    "        \"alignments\": [\n",
    "            {\"comment_token\": [\"token1\", \"token2\"], \"code_token\": [\"tokenA\", \"tokenB\"]},\n",
    "            {\"comment_token\": [\"token3\", \"token4\"], \"code_token\": [\"tokenC\", \"tokenD\"]},\n",
    "            {\"comment_token\": [\"token5\", \"token6\"], \"code_token\": [\"tokenE\", \"tokenF\"]}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    student_prompt = f\"\"\"\n",
    "    CRITICAL ALIGNMENT INSTRUCTIONS:\n",
    "    1. FIRST analyze comment token types:\n",
    "       - Identify VERBS (actions, operations)\n",
    "       - Identify NOUNS (objects, concepts)\n",
    "       - Keep these categories separate\n",
    "       - Each comment token can ONLY be used ONCE in matching\n",
    "    2. THEN categorize code tokens:\n",
    "       - FUNCTION NAMES (methods, routines)\n",
    "       - VARIABLE NAMES (parameters, fields)\n",
    "       - API CALLS (library functions)\n",
    "       - KEYWORDS (control flow, operators)\n",
    "       - Each code token can ONLY be matched ONCE\n",
    "    3. ENFORCE concept extraction rules:\n",
    "       - Extract concepts ONLY from comment VERBS and NOUNS\n",
    "       - Each comment token can only form ONE concept\n",
    "       - Ignore modifiers and other parts of speech\n",
    "       - Ensure concepts are semantically meaningful\n",
    "    4. Follow matching principles:\n",
    "       - Each code token can only match ONE concept\n",
    "       - Match concepts to most semantically similar code tokens\n",
    "    5. MAXIMIZE valid concept-code matches:\n",
    "       - Choose strongest semantic matches between concepts and code\n",
    "       - Leave concepts/tokens unmatched rather than force weak matches\n",
    "\n",
    "    Here are the tokens to align:\n",
    "    {student_tokens_part}\n",
    "\n",
    "    Based on the above instructions and following the teacher example, provide comprehensive alignments between comment concepts and code implementations. Output in this format:\n",
    "    {alignment_format}\n",
    "    \"\"\"\n",
    "\n",
    "    promt_str = system_prompt + teacher_prompt + student_prompt\n",
    "\n",
    "    client = OpenAI(base_url=os.environ.get(\"OPENAI_BASE_URL\"))\n",
    "\n",
    "    # Print logs directly in notebook\n",
    "    print(f\"\\n=== Processing auto_label_ind: {auto_label_ind} ===\")\n",
    "    for i, teacher in enumerate(teachers):\n",
    "        print(f\"Teacher {i} idx: {teacher['teacher_idx']}, confidence: {teacher['confidence']}\")\n",
    "    print(f\"Student idx: {student_idx}\")\n",
    "    print(f\"Student tokens: {student_tokens_part}\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[{\"role\": \"user\", \"content\": promt_str}],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\", \n",
    "            \"json_schema\": {\n",
    "                \"strict\": True,\n",
    "                \"name\": \"alignment_response\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"alignments\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"comment_token\": {\n",
    "                                        \"type\": \"array\",\n",
    "                                        \"items\": {\"type\": \"string\"}\n",
    "                                    },\n",
    "                                    \"code_token\": {\n",
    "                                        \"type\": \"array\", \n",
    "                                        \"items\": {\"type\": \"string\"}\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\"comment_token\", \"code_token\"],\n",
    "                                \"additionalProperties\": False\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"alignments\"],\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        max_tokens=500)\n",
    "\n",
    "    # Print response content\n",
    "    response_content = response.choices[0].message.content\n",
    "    print(f\"Response content:\\n{response_content}\")\n",
    "\n",
    "    # Parse JSON response\n",
    "    json_content = re.search(r'\\{.*\\}', response_content, re.DOTALL)\n",
    "    if json_content:\n",
    "        json_str = json_content.group(0)\n",
    "        try:\n",
    "            alignment_output = json.loads(json_str)\n",
    "            print(\"Successfully parsed JSON response\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON parsing error: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(\"No valid JSON content found\")\n",
    "        continue\n",
    "\n",
    "    final_results = []\n",
    "\n",
    "    # Initialize sets to track matched indices\n",
    "    matched_comment_indices = set()\n",
    "    matched_code_indices = set()\n",
    "    has_hallucination = False # Track hallucination at global level\n",
    "    \n",
    "    # Track matched tokens per alignment round\n",
    "    matched_token_texts = {} # Dict mapping alignment index to set of token texts\n",
    "    \n",
    "    for alignment_idx, alignment in enumerate(alignment_output[\"alignments\"]):\n",
    "        comment_matches = []\n",
    "        code_matches = []\n",
    "        code_current_pos = 0\n",
    "        comment_current_pos = 0\n",
    "        \n",
    "        # Initialize set for this alignment round\n",
    "        matched_token_texts[alignment_idx] = set()\n",
    "        \n",
    "        # Process comment tokens\n",
    "        print(\"\\nProcessing comment tokens:\")\n",
    "        for token in alignment[\"comment_token\"]:\n",
    "            # Remove special characters (triangles and squares) from token\n",
    "            clean_token = re.sub(r'[▲■].*$', '', token)\n",
    "            print(f\"Original token: {token}, Cleaned token: {clean_token}\")\n",
    "            \n",
    "            # Find first valid continuous match starting from last match position\n",
    "            comment_token_indices = []\n",
    "            last_match_pos = comment_current_pos if comment_current_pos > 0 else 0\n",
    "            \n",
    "            # Try to find first valid continuous match starting from last match position\n",
    "            i = last_match_pos\n",
    "            found_match = False\n",
    "            while i < len(student_docstring_tokens_raw) and not found_match:\n",
    "                # Get continuous tokens starting at position i\n",
    "                continuous_token = \"\"\n",
    "                j = i\n",
    "                while j < len(student_docstring_tokens_raw):\n",
    "                    continuous_token += student_docstring_tokens_raw[j].replace('Ġ', '')\n",
    "                    if clean_token == continuous_token:\n",
    "                        # Check if any index in range is already matched\n",
    "                        indices_range = set(range(i, j + 1))\n",
    "                        if not indices_range.intersection(matched_comment_indices):\n",
    "                            comment_token_indices.extend(range(i, j + 1))\n",
    "                            comment_current_pos = j + 1\n",
    "                            matched_comment_indices.update(indices_range)\n",
    "                            print(f\"Match found from index {i} to {j}\")\n",
    "                            found_match = True\n",
    "                        break\n",
    "                    j += 1\n",
    "                if not found_match:\n",
    "                    i += 1\n",
    "                \n",
    "            comment_matches.extend(comment_token_indices)\n",
    "        print(f\"Final comment matches: {comment_matches}\")\n",
    "        \n",
    "        # Process code tokens with new matching logic\n",
    "        print(\"\\nProcessing code tokens:\")\n",
    "        \n",
    "        for code_token in alignment[\"code_token\"]:\n",
    "            # Check for hallucination by looking up token in unique tokens\n",
    "            try:\n",
    "                token_idx = code_tokens_further_data_unique[student_idx].index(code_token)\n",
    "                print(f\"Found token {code_token} at index {token_idx}\")\n",
    "            except ValueError:\n",
    "                print(f\"Could not find token {code_token} in unique tokens - hallucination detected\")\n",
    "                has_hallucination = True\n",
    "                break\n",
    "                \n",
    "            # Find which original token this was derived from using code_token_maps\n",
    "            for orig_idx, derived_indices in code_token_maps[student_idx].items():\n",
    "                if token_idx in derived_indices:\n",
    "                    # Convert orig_idx from string to integer before using it\n",
    "                    orig_idx_int = int(orig_idx)\n",
    "                    print(f\"Original key {orig_idx} was converted to integer {orig_idx_int}\")\n",
    "                    # Get the tokenized range for this original token\n",
    "                    try:\n",
    "                        tokenize_range = tokenized_id_data[student_idx][1:][orig_idx_int]\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                    print(f\"Original token at index {orig_idx} was tokenized to range {tokenize_range}\")\n",
    "                    \n",
    "                    # Get the clean version of the unique token\n",
    "                    clean_token = re.sub(r'[▲■].*$', '', code_token)\n",
    "                    \n",
    "                    # Try to match the clean token against the tokenized pieces\n",
    "                    start_idx = tokenize_range[0]  # Keep original [a,b] range\n",
    "                    end_idx = tokenize_range[1] + 1\n",
    "                    \n",
    "                    # Get the tokens in the range, including start index\n",
    "                    range_tokens = code_tokens_strs[student_idx][start_idx+1:end_idx]\n",
    "                    print(f\"Range tokens: {range_tokens}\")\n",
    "                    \n",
    "                    # Try to find minimal token combinations that match clean_token\n",
    "                    i = 0\n",
    "                    found_any_match = False\n",
    "                    while i < len(range_tokens):\n",
    "                        # Try combining minimal number of tokens starting at position i\n",
    "                        min_combined = \"\"\n",
    "                        j = i\n",
    "                        min_matched_indices = []\n",
    "                        found_match = False\n",
    "                        \n",
    "                        while j < len(range_tokens):\n",
    "                            curr_token = range_tokens[j].replace('Ġ', '')\n",
    "                            min_combined += curr_token\n",
    "                            min_matched_indices.extend(range(start_idx+i, start_idx+j+1))\n",
    "                            \n",
    "                            # Check for exact match with minimal tokens\n",
    "                            if min_combined == clean_token:\n",
    "                                indices_range = set(min_matched_indices)\n",
    "                                # Only match if tokens haven't been used before\n",
    "                                if not indices_range.intersection(matched_code_indices):\n",
    "                                    code_matches.extend(list(indices_range))  # Convert to list to extend\n",
    "                                    matched_code_indices.update(indices_range)\n",
    "                                    # Add matched text to current alignment's set\n",
    "                                    matched_token_texts[alignment_idx].add(min_combined)\n",
    "                                    print(f\"Added minimal indices {list(indices_range)} for combined token {min_combined}\")\n",
    "                                    found_match = True\n",
    "                                    found_any_match = True\n",
    "                                    break\n",
    "                            # Stop if combined tokens exceed clean_token length\n",
    "                            elif len(min_combined) > len(clean_token):\n",
    "                                break\n",
    "                                \n",
    "                            j += 1\n",
    "                            \n",
    "                        if found_match:\n",
    "                            break\n",
    "                        i += 1\n",
    "\n",
    "                    # If no matches found for any token in this range, match all range tokens to first clean token\n",
    "                    if not found_any_match:\n",
    "                        all_indices = list(range(start_idx, end_idx-1))\n",
    "                        available_indices = [i for i in all_indices if i not in matched_code_indices]\n",
    "                        if available_indices:\n",
    "                            code_matches.extend(available_indices)\n",
    "                            matched_code_indices.update(available_indices)\n",
    "                            combined_token = ''.join([t.replace('Ġ', '') for t in range_tokens])\n",
    "                            matched_token_texts[alignment_idx].add(combined_token)\n",
    "                            print(f\"No individual matches found - matching available indices {available_indices} for combined token {combined_token} to {clean_token}\")\n",
    "                    break\n",
    "        \n",
    "        # If hallucination detected, skip this alignment\n",
    "        if has_hallucination:\n",
    "            print(\"Hallucination detected - skipping all alignments\")\n",
    "            final_results = [] # Clear all previous results\n",
    "            break\n",
    "                \n",
    "        print(f\"Final code matches: {code_matches}\")\n",
    "        \n",
    "        # Keep indices for now\n",
    "        if len(comment_matches) > 0 and len(code_matches) > 0:\n",
    "            final_results.append([comment_matches, code_matches])\n",
    "            print(f\"Added to final results: {[comment_matches, code_matches]}\")\n",
    "\n",
    "    # Post-validation: Find identical unmatched tokens\n",
    "    for i in range(len(code_tokens_strs[student_idx])):\n",
    "        if i-1 not in matched_code_indices:\n",
    "            # Get the token text and skip special tokens\n",
    "            token_text = code_tokens_strs[student_idx][i].replace('Ġ', '')\n",
    "            # Skip common special characters/operators\n",
    "            if token_text in ['.', '_', ':', ';', ',', '(', ')', '[', ']', '{', '}', \n",
    "                            '+', '-', '*', '/', '=', '<', '>', '!', '&', '|', '^',\n",
    "                            'self', 'def', '<unk>']:\n",
    "                continue\n",
    "                \n",
    "            # Check if token appears in any previous alignment rounds\n",
    "            appearing_rounds = []\n",
    "            for round_idx, token_set in matched_token_texts.items():\n",
    "                if token_text in token_set:\n",
    "                    appearing_rounds.append(round_idx)\n",
    "            \n",
    "            if appearing_rounds:\n",
    "                if len(appearing_rounds) > 1:\n",
    "                    # Token appears in multiple rounds - invalidate results\n",
    "                    print(f\"Token {token_text} appears in multiple rounds {appearing_rounds} - invalidating results\")\n",
    "                    final_results = []\n",
    "                    break\n",
    "                else:\n",
    "                    # Token appears in exactly one round - add to that round's matches\n",
    "                    round_idx = appearing_rounds[0]\n",
    "                    \n",
    "                    # Check if round_idx is valid\n",
    "                    if round_idx >= len(final_results):\n",
    "                        print(f\"Invalid round index {round_idx} - skipping token {token_text}\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Get the existing indices for this round\n",
    "                    try:\n",
    "                        comment_indices = final_results[round_idx][0]\n",
    "                        code_indices = final_results[round_idx][1]\n",
    "                    except IndexError:\n",
    "                        print(f\"Invalid final results format for round {round_idx} - skipping token {token_text}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Add the new code index\n",
    "                    code_indices.append(i-1)\n",
    "                    \n",
    "                    # Update the final results with new code indices\n",
    "                    final_results[round_idx] = [comment_indices, code_indices]\n",
    "                    \n",
    "                    matched_code_indices.add(i-1)\n",
    "                    print(f\"Post-validation: Added identical unmatched token at index {i-1}: {token_text} to round {round_idx}\")\n",
    "                    \n",
    "    # Convert indices to intervals for each round at the end\n",
    "    final_intervals = []\n",
    "    for comment_indices, code_indices in final_results:\n",
    "        comment_intervals = convert_to_intervals(sorted(comment_indices))\n",
    "        code_intervals = convert_to_intervals(sorted(code_indices))\n",
    "        final_intervals.append([comment_intervals, code_intervals])\n",
    "        print(f\"\\nConverted to intervals:\\nComment intervals: {comment_intervals}\\nCode intervals: {code_intervals}\")\n",
    "    \n",
    "    final_results = final_intervals\n",
    "\n",
    "    if not final_results:\n",
    "        print(f\"No final results for auto_label_ind: {auto_label_ind}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Final results: {final_results}\")\n",
    "\n",
    "    new_entry = {\n",
    "        \"idx\": int(student_idx),\n",
    "        \"match\": final_results\n",
    "    }\n",
    "\n",
    "    file_path = \"/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/sorted_labelling_sample_api_student_conf.jsonl\"\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(json.dumps(new_entry, separators=(',',':'), ensure_ascii=False) + '\\n')\n",
    "        print(\"Successfully wrote new entry to JSONL file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted data has been written to /home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/sorted_labelling_sample_api_student_conf_sorted_2.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 输入文件路径和输出文件路径\n",
    "input_file = '/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/sorted_labelling_sample_api_student_conf_2.jsonl'\n",
    "output_file = '/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/sorted_labelling_sample_api_student_conf_sorted_2.jsonl'\n",
    "\n",
    "# 读取 .jsonl 文件，按 idx 排序\n",
    "with open(input_file, 'r') as f:\n",
    "    # 逐行读取并解析 JSON\n",
    "    data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# 按 idx 进行排序\n",
    "sorted_data = sorted(data, key=lambda x: x['idx'])\n",
    "\n",
    "# 将排序后的数据写入新的文件\n",
    "with open(output_file, 'w') as f:\n",
    "    for item in sorted_data:\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Sorted data has been written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data has been written back to /home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/sorted_labelling_sample_api_student_conf_sorted_2.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 文件路径\n",
    "file_path = '/home/yiming/cophi/projects/fork/CodeBERT/GraphCodeBERT/codesearch/auto_labelling/sorted_labelling_sample_api_student_conf_sorted_2.jsonl'\n",
    "\n",
    "# 设置索引限制\n",
    "comment_max_idx = 127\n",
    "code_max_idx = 255\n",
    "\n",
    "# 读取原文件并处理每一行\n",
    "with open(file_path, 'r') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        item = json.loads(line.strip())\n",
    "        \n",
    "        # 新的 match 列表，用来存储过滤后的有效 match\n",
    "        new_match = []\n",
    "        \n",
    "        # 遍历每个 match 对象，检查并过滤其中的区间\n",
    "        for match_pair in item['match']:\n",
    "            # match_pair 是一个包含两个区间的列表，分别是 comment_range 和 code_range\n",
    "            comment_range, code_range = match_pair\n",
    "            \n",
    "            # 过滤 comment_range 和 code_range 中超过最大索引的区间\n",
    "            filtered_comment_range = []\n",
    "            filtered_code_range = []\n",
    "            \n",
    "            # 每两个元素表示一个区间\n",
    "            for i in range(0, len(comment_range), 2):\n",
    "                start, end = comment_range[i], comment_range[i + 1]\n",
    "                if start <= comment_max_idx and end <= comment_max_idx:\n",
    "                    filtered_comment_range.extend([start, end])\n",
    "\n",
    "            for i in range(0, len(code_range), 2):\n",
    "                start, end = code_range[i], code_range[i + 1]\n",
    "                if start <= code_max_idx and end <= code_max_idx:\n",
    "                    filtered_code_range.extend([start, end])\n",
    "\n",
    "            # 只有当 comment_range 和 code_range 都不为空时，才保留这一对\n",
    "            if filtered_comment_range and filtered_code_range:\n",
    "                new_match.append([filtered_comment_range, filtered_code_range])\n",
    "        \n",
    "        # 如果新 match 列表有内容，更新 item 中的 match 字段\n",
    "        if new_match:\n",
    "            item['match'] = new_match\n",
    "            data.append(item)\n",
    "\n",
    "# 将修改后的数据重新写入文件\n",
    "with open(file_path, 'w') as f:\n",
    "    for item in data:\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Processed data has been written back to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autolabel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
